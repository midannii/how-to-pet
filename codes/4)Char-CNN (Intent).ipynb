{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reuse of https://github.com/TensorMSA/tensormsa_jupyter/blob/master/chap13_chatbot_lecture/4.Char-CNN%20(Intent).ipynb\n",
    "\n",
    "    - change `tf` ver1 -> ver 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.image import imread, imsave \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import word2vec\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['말티즈 성격 어때요?', '자꾸 털이 빠지고 기침을 하네 ㅠㅠ', '지금8개월인데예방접종뭐해야해?', '초콜릿 먹여도 되냐?']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = 50\n",
    "encode_length = 4\n",
    "label_size = 4\n",
    "embed_type = \"onehot\" #onehot or w2v\n",
    "# Choose single test\n",
    "# filter_type = \"single\"\n",
    "# filter_number = 32\n",
    "# filter_size = 2\n",
    "\n",
    "# Choose multi test\n",
    "filter_type = \"multi\"\n",
    "filter_sizes = [2,3,4,2,3,4,2,3,4]\n",
    "num_filters = len(filter_sizes)\n",
    "\n",
    "train_data_list =  {\n",
    "                'encode' : ['말티즈 성격 어때요?','자꾸 털이 빠지고 기침을 하네 ㅠㅠ','지금8개월인데예방접종뭐해야해?', '초콜릿 먹여도 되냐?'],\n",
    "                'decode' : ['0','1','2','3']\n",
    "             }\n",
    "train_data_list.get('encode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train_vector_model(str_buf):\n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    str_buf = train_data_list['encode']\n",
    "    pos1 = mecab.pos(''.join(str_buf))\n",
    "    pos2 = ' '.join(list(map(lambda x : '\\n' if x[1] in ['SF'] else x[0], pos1))).split('\\n')\n",
    "    morphs = list(map(lambda x : mecab.morphs(x) , pos2))\n",
    "    #print(str_buf)\n",
    "    return morhps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_vector_model(morphs):\n",
    "    model = word2vec.Word2Vec(size=vector_size, window=2, min_count=1)\n",
    "    model.build_vocab(morphs)\n",
    "    model.train(morphs, epochs=model.iter, total_examples=model.corpus_count)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['말', '티즈', '성격', '어때요'], ['자꾸', '털', '이', '빠지', '고', '기침', '을', '하', '네', 'ㅠㅠ', '지금', '8', '개월', '인데', '예방', '접종', '뭐', '해야', '해'], ['초콜릿', '먹여', '도', '되', '냐'], []]\n"
     ]
    }
   ],
   "source": [
    "temp = pre_train_vector_model(train_data_list)\n",
    "# pos 1\n",
    "## [('말', 'NNG'), ('티즈', 'NNP'), ('성격', 'NNG'), ('어때요', 'VA+EF'), ('?', 'SF'), ('자꾸', 'MAG'), ('털', 'NNG'), ('이', 'JKS'), \n",
    "##    ('빠지', 'VV'), ('고', 'EC'), ('기침', 'NNG'), ('을', 'JKO'), ('하', 'VV'), ('네', 'EF'), ('ㅠㅠ', 'UNKNOWN'), ('지금', 'MAG'), \n",
    "##    ('8', 'SN'), ('개월', 'NNBC'), ('인데', 'VCP+EC'), ('예방', 'NNG'), ('접종', 'NNG'), ('뭐', 'IC'), ('해야', 'VV+EC'), ('해', 'VX+EF'), \n",
    "##    ('?', 'SF'), ('초콜릿', 'NNG'), ('먹여', 'VV+EC'), ('도', 'JX'), ('되', 'VV'), ('냐', 'EF'), ('?', 'SF')]\n",
    "\n",
    "# pos 2\n",
    "## ['말 티즈 성격 어때요 ', ' 자꾸 털 이 빠지 고 기침 을 하 네 ㅠㅠ 지금 8 개월 인데 예방 접종 뭐 해야 해 ', ' 초콜릿 먹여 도 되 냐 ', '']\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=28, size=50, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model = train_vector_model(temp)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(data_path):\n",
    "    df_csv_read = pd.DataFrame(data_path)\n",
    "    return df_csv_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def embed(encode_raw) : \n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for encode_raw in data['encode'] : \n",
    "        encode_raw = mecab.morphs(encode_raw)\n",
    "        encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
    "        if(embed_type == 'onehot') :\n",
    "            bucket = np.zeros(vector_size, dtype=float).copy()\n",
    "            input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float), encode_raw)))\n",
    "        else : \n",
    "            input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "        inputs.append(input.flatten())\n",
    "        \n",
    "    for decode_raw in data['decode']: \n",
    "        label = np.zeros(label_size, dtype=float)\n",
    "        np.put(label, decode_raw, 1)\n",
    "        labels.append(label)\n",
    "    return inputs, labels\n",
    "\n",
    "def onehot_vectorize(bucket, x):\n",
    "    np.put(bucket, model.wv.index2word.index(x),1)\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed word to vector on predict step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_embed(data) : \n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    encode_raw = mecab.morphs(data)\n",
    "    encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
    "    if(embed_type == 'onehot') :\n",
    "        bucket = np.zeros(vector_size, dtype=float).copy()\n",
    "        input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "    else : \n",
    "        input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get train and test data for feed on tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    train_data, train_label = embed(load_csv(train_data_list))\n",
    "    test_data, test_label = embed(load_csv(train_data_list))\n",
    "    return train_label, test_label, train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create graph with single filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define cnn graph func\n"
     ]
    }
   ],
   "source": [
    "def create_s_graph(train=True):\n",
    "    # placeholder is used for feeding data.\n",
    "    x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
    "    y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
    "\n",
    "    # reshape input data\n",
    "    x_image = tf.reshape(x, [-1,encode_length,vector_size,1], name=\"x_image\")\n",
    "    \n",
    "    # Build a convolutional layer and maxpooling with random initialization\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([filter_size, filter_size, 1, filter_number], stddev=0.1), name=\"W_conv1\") # W is [row, col, channel, feature]\n",
    "    b_conv1 = tf.Variable(tf.zeros([filter_number]), name=\"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1, name=\"h_conv1\")\n",
    "    h_pool1 = tf.nn.max_pool( h_conv1 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool1\")\n",
    "    \n",
    "    # Build a fully connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool1, [-1, int((encode_length/2)*(vector_size/2))*filter_number], name=\"h_pool2_flat\")\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([int((encode_length/2)*(vector_size/2))*filter_number, 256], stddev=0.1), name = 'W_fc1')\n",
    "    b_fc1 = tf.Variable(tf.zeros([256]), name = 'b_fc1')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "    \n",
    "    keep_prob = 1.0\n",
    "    if(train) : \n",
    "        # Dropout Layer\n",
    "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
    "    \n",
    "    # Build a fully connected layer with softmax \n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([256, label_size], stddev=0.1), name = 'W_fc2')\n",
    "    b_fc2 = tf.Variable(tf.zeros([label_size]), name = 'b_fc2')\n",
    "    #y=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=\"y\")\n",
    "    y=tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    # define the Loss function\n",
    "    #cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target))\n",
    "    \n",
    "    # define optimization algorithm\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "    # correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # tf.cast() : changes true -> 1 / false -> 0\n",
    "    # tf.reduce_mean() : calculate the mean\n",
    "    \n",
    "    return accuracy, x, y_target, keep_prob, train\n",
    "\n",
    "print(\"define cnn graph func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create graph with multi filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the import statements \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Change it to the sample expression as follows. \n",
    "init = tf.compat.v1.random.truncated_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define cnn graph func\n"
     ]
    }
   ],
   "source": [
    "# tf1, but still have change for contrib \n",
    "\n",
    "def create_m_graph(train=True):\n",
    "    # placeholder is used for feeding data.\n",
    "    x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
    "    y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
    "\n",
    "    # reshape input data\n",
    "    x_image = tf.reshape(x, [-1,encode_length,vector_size,1], name=\"x_image\")\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [filter_size, vector_size, 1, num_filters]\n",
    "            W_conv1 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            \n",
    "            conv = tf.nn.conv2d(\n",
    "                x_image,\n",
    "                W_conv1,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            \n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b_conv1), name=\"relu\")\n",
    "            # Maxpooling over the outputs\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, encode_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "    # Combine all the pooled features\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool = tf.concat(pooled_outputs, 3)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    " \n",
    "    # Add dropout\n",
    "    keep_prob = 1.0\n",
    "    if(train) : \n",
    "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "        h_pool_flat = tf.nn.dropout(h_pool_flat, keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    #W_fc1 = tf.get_variable(\n",
    "    #    \"W_fc1\",\n",
    "    #    shape=[num_filters_total, label_size],\n",
    "    #    initializer=tf.contrib.layers.xavier_initializer())\n",
    "    initializer = tf.initializers.glorot_uniform()\n",
    "    W_fc1 = tf.compat.v1.get_variable(\n",
    "        \"W_fc1\",\n",
    "        initializer = initializer(shape=(num_filters_total, label_size)))\n",
    "    \n",
    "    \n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[label_size]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W_fc1)\n",
    "    l2_loss += tf.nn.l2_loss(b_fc1)\n",
    "    y = tf.nn.xw_plus_b(h_pool_flat, W_fc1, b_fc1, name=\"scores\")\n",
    "    predictions = tf.argmax(y, 1, name=\"predictions\")\n",
    "\n",
    "    # CalculateMean cross-entropy loss\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target)\n",
    "    cross_entropy = tf.reduce_mean(losses)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(y_target, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    return accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1\n",
    "    \n",
    "print(\"define cnn graph func\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define cnn graph func\n"
     ]
    }
   ],
   "source": [
    "## tf2 \n",
    "\n",
    "def create_m_graph(train=True):\n",
    "    # placeholder is used for feeding data. - tensor 2에서부터는 쓰이지 않음 \n",
    "    #x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
    "    #y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
    "    \n",
    "    x = tf.keras.Input(shape=(None, encode_length * vector_size), dtype=tf.dtypes.float32)\n",
    "    y_target = tf.constant([label_size, label_size], dtype='float')\n",
    "    \n",
    "    # reshape input data\n",
    "    x_image = tf.reshape(x, [-1,encode_length, vector_size,1], name=\"x_image\")\n",
    "    \n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [filter_size, vector_size, 1, num_filters]\n",
    "            #W_conv1 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            W_conv1 = tf.Variable(tf.compat.v1.random.truncated_normal(filter_shape, stddev=0.1), name=\"W\", dtype=tf.dtypes.float32)\n",
    "            b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            \n",
    "            conv = tf.nn.conv2d(\n",
    "                x_image,\n",
    "                W_conv1,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            \n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b_conv1), name=\"relu\")\n",
    "            # Maxpooling over the outputs\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, encode_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "    # Combine all the pooled features\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool = tf.concat(pooled_outputs, 3)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    " \n",
    "    # Add dropout\n",
    "    keep_prob = 1.0\n",
    "    if(train) : \n",
    "        #keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "        keep_prob = tf.Variable(tf.ones(shape = [keep_prob,]))\n",
    "        h_pool_flat = tf.nn.dropout(h_pool_flat, keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    W_fc1 = tf.get_variable(\n",
    "        \"W_fc1\",\n",
    "        shape=[num_filters_total, label_size],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[label_size]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W_fc1)\n",
    "    l2_loss += tf.nn.l2_loss(b_fc1)\n",
    "    y = tf.nn.xw_plus_b(h_pool_flat, W_fc1, b_fc1, name=\"scores\")\n",
    "    predictions = tf.argmax(y, 1, name=\"predictions\")\n",
    "\n",
    "    # CalculateMean cross-entropy loss\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target)\n",
    "    cross_entropy = tf.reduce_mean(losses)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(y_target, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    return accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1\n",
    "    \n",
    "print(\"define cnn graph func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize weight matrix function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_layer(weight_list) :\n",
    "    if(filter_type == 'multi') : \n",
    "        show = np.array(weight_list).reshape(num_filters, filter_sizes[np.argmax(filter_sizes)], vector_size)\n",
    "        for i, matrix in enumerate(show) :\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(matrix)\n",
    "        plt.show()\n",
    "    else : \n",
    "        show = np.array(weight_list).reshape(32, 2, 2)\n",
    "        for i, matrix in enumerate(show) :\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(matrix)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train, labels_test, data_filter_train, data_filter_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로 1.x\n",
    "outputs = session.run(f(placeholder), feed_dict={placeholder: input})\n",
    "# 텐서플로 2.0\n",
    "outputs = f(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-179-7dea0a822114>:21: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "step 0, training accuracy: 0.250\n",
      "step 10, training accuracy: 0.500\n",
      "step 20, training accuracy: 0.750\n",
      "step 30, training accuracy: 0.750\n",
      "step 40, training accuracy: 0.750\n",
      "step 50, training accuracy: 0.750\n",
      "step 60, training accuracy: 0.750\n",
      "step 70, training accuracy: 0.750\n",
      "step 80, training accuracy: 0.750\n",
      "step 90, training accuracy: 1.000\n",
      "step 100, training accuracy: 1.000\n",
      "step 110, training accuracy: 1.000\n",
      "step 120, training accuracy: 1.000\n",
      "step 130, training accuracy: 1.000\n",
      "step 140, training accuracy: 1.000\n",
      "step 150, training accuracy: 1.000\n",
      "step 160, training accuracy: 1.000\n",
      "step 170, training accuracy: 1.000\n",
      "step 180, training accuracy: 1.000\n",
      "step 190, training accuracy: 1.000\n",
      "test accuracy: 1\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "def run() : \n",
    "    try : \n",
    "        # get Data \n",
    "        labels_train, labels_test, data_filter_train, data_filter_test = get_test_data()\n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    " \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        if(filter_type == 'single') :\n",
    "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_s_graph(train=True)\n",
    "        else :\n",
    "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_m_graph(train=True)\n",
    "            \n",
    "        # set saver\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        # training the MLP\n",
    "        for i in range(200): \n",
    "            sess.run(train_step, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 0.5})\n",
    "            if i%10 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
    "                print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
    "                \n",
    "        # for given x, y_target data set\n",
    "        print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x:data_filter_test, y_target: labels_test, keep_prob: 1}))\n",
    "        \n",
    "        # show weight matrix as image \n",
    "        weight_vectors = sess.run(W_conv1, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 1.0})\n",
    "        #show_layer(weight_vectors)\n",
    "        \n",
    "        # Save Model\n",
    "        path = './model/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(\"path created\")\n",
    "        saver.save(sess, path)\n",
    "        print(\"model saved\")\n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()\n",
    "\n",
    "# run stuff\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## ver of tf 2\n",
    "\n",
    "def run() : \n",
    "    try : \n",
    "        # get Data \n",
    "        #labels_train, labels_test, data_filter_train, data_filter_test = get_test_data()\n",
    "        \n",
    "        # reset Graph\n",
    "        ops.reset_default_graph()   \n",
    " \n",
    "        # Create Session - ver2 부터 session 지정 생략\n",
    "        # sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        \n",
    "        # create graph\n",
    "        if(filter_type == 'single') :\n",
    "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_s_graph(train=True)\n",
    "        else :\n",
    "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_m_graph(train=True)\n",
    "            \n",
    "        # set saver\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        # initialize the variables\n",
    "        # sess.run(tf.global_variables_initializer())\n",
    "        tf.global_variables_initializer()\n",
    "    \n",
    "        # training the MLP\n",
    "        for i in range(200): \n",
    "            ##sess.run(train_step, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 0.5})\n",
    "            tf.run(train_step, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 0.5})\n",
    "            if i%10 == 0:\n",
    "                #train_accuracy = sess.run(accuracy, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
    "                ######train_accuracy = \n",
    "                print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
    "                \n",
    "        # for given x, y_target data set\n",
    "        #  print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x:data_filter_test, y_target: labels_test, keep_prob: 1}))\n",
    "        print  (\"test accuracy: %g\"% tf.run(accuracy, feed_dict={x:data_filter_test, y_target: labels_test, keep_prob: 1}))\n",
    "        \n",
    "        # show weight matrix as image \n",
    "        # weight_vectors = sess.run(W_conv1, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 1.0})\n",
    "        ######weight_vectors = \n",
    "        \n",
    "        #show_layer(weight_vectors)\n",
    "        \n",
    "        # Save Model\n",
    "        path = './model/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(\"path created\")\n",
    "        saver.save(sess, path)\n",
    "        print(\"model saved\")\n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    #finally :\n",
    "     #   sess.close()\n",
    "\n",
    "# run stuff\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(test_data) : \n",
    "    try : \n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        if(filter_type == 'single') :\n",
    "            _, x, _, _, _, y, _, _ = create_s_graph(train=False)\n",
    "        else : \n",
    "            _, x, _, _, _, y, _, _ = create_m_graph(train=False)\n",
    "        \n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # set saver\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Restore Model\n",
    "        path = './model/'\n",
    "        if os.path.exists(path):\n",
    "            saver.restore(sess, path)\n",
    "            print(\"model restored\")\n",
    "\n",
    "        # training the MLP\n",
    "        #print(\"input data : {0}\".format(test_data))\n",
    "        y = sess.run([y], feed_dict={x: np.array([test_data])})\n",
    "        print(\"result : {0}\".format(y))\n",
    "        print(\"result : {0}\".format(np.argmax(y)))\n",
    "        \n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in dict : ['말', '티즈', '성격', '어때요', '자꾸', '털', '이', '빠지', '고', '기침', '을', '하', '네', 'ㅠㅠ', '지금', '8', '개월', '인데', '예방', '접종', '뭐', '해야', '해', '초콜릿', '먹여', '도', '되', '냐']\n"
     ]
    }
   ],
   "source": [
    "print(\"words in dict : {0}\".format(model.wv.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "model restored\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "error on training: could not convert string to float: '말'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-c1f3215e35e5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(test_data)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print(\"input data : {0}\".format(test_data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result : {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1149\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '말'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-341a5f99f071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-c1f3215e35e5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(test_data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"error on training: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mfinally\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: error on training: could not convert string to float: '말'"
     ]
    }
   ],
   "source": [
    "predict(np.array(temp[0]).flatten())\n",
    "predict(np.array(temp[1]).flatten())\n",
    "predict(np.array(temp[2]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
